<!DOCTYPE html>
<html class="w-full h-full">

<head>
  <title>Kalosm 0.4 - Floneum Blog</title>
  <meta name="description" content="Floneum is a graph editor for local LLM workflows." />
  <meta content="text/html;charset=utf-8" http-equiv="Content-Type" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta charset="UTF-8" />
  <style>
    .markdown-body ul {
      list-style: disc;
    }

    .markdown-body ol {
      list-style: decimal;
    }

    .markdown-body li {
      display: list-item;
    }

    .markdown-body button {
      display: inline-block;
      background-color: rgba(209, 213, 219, 0.3);
      border-radius: 0.25rem;
      padding: 0.25rem 0.5rem;
      border: 1px solid;
      margin: 0.25rem;
    }

    .markdown-body .header {
      color: inherit
    }

    .markdown-body {
      box-sizing: border-box;
      min-width: 200px;
      max-width: 980px;
      margin: 0 5%;
      /* padding: 45px; */
      list-style: disc;
    }

    .main::-webkit-scrollbar {
      display: none;
    }

    /* Hide scrollbar for IE, Edge and Firefox */
    .main {
      -ms-overflow-style: none;
      /* IE and Edge */
      scrollbar-width: none;
      /* Firefox */
    }
  </style>
  <script src="https://buttons.github.io/buttons.js"></script>
<link rel="preload" as="script" href="/./assets/floneum-site-ce954da0ae683c0e.js" crossorigin><link rel="preload" as="fetch" type="application/wasm" href="/./assets/floneum-site_bg-d8c3438104baad75.wasm" crossorigin><link rel="stylesheet" href="/assets/output-956cf6bd1a90dc22.css"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/5.1.0/github-markdown-light.min.css"/><link rel="icon" href="/assets/Icon-7abfef476d4b581a.png" type="image/png"/></head>

<body style="overflow-x:hidden;--scroll:0;" class="w-full h-full">
  <div id="main" class="w-full h-full"><script>window.hydrate_queue=[];window.dx_hydrate=(id,data,debug_types,debug_locations)=>{const decoded=atob(data),bytes=Uint8Array.from(decoded,(c)=>c.charCodeAt(0));if(window.hydration_callback)window.hydration_callback(id,bytes,debug_types,debug_locations);else window.hydrate_queue.push([id,bytes,debug_types,debug_locations])};
</script><!--placeholder0--><!--placeholder1--><!--placeholder2--><!--placeholder3--><nav class="bg-white shadow" data-node-hydration="4"><div class="mx-auto max-w-7xl px-2 sm:px-4 lg:px-8"><div class="flex h-16 justify-between"><div class="flex px-2 lg:px-0"><div class="flex flex-shrink-0 items-center"><a href="/" class="text-xl m-2 md:mr-12 flex flex-row items-center" data-node-hydration="5,click:1"><img src="/assets/Icon-7abfef476d4b581a.png" class="h-8 w-8 mx-2" alt="Floneum" data-node-hydration="6"/><!--node-id7-->Floneum<!--#--></a></div><div class="hidden lg:ml-6 lg:flex lg:space-x-8"><a href="/" class="inline-flex items-center border-b-2 border-transparent px-1 pt-1 text-sm font-medium text-gray-500 hover:border-gray-300 hover:text-gray-700" data-node-hydration="8,click:1"><!--node-id9-->Home<!--#--></a><a href="/docs/" class="inline-flex items-center border-b-2 border-transparent px-1 pt-1 text-sm font-medium text-gray-500 hover:border-gray-300 hover:text-gray-700" data-node-hydration="10,click:1"><!--node-id11-->Docs<!--#--></a></div></div><div class="flex flex-1 items-center justify-center px-2 lg:ml-6 lg:justify-end"><div class="w-full max-w-lg lg:max-w-xs"><label for="search" class="sr-only" data-node-hydration="12">Search</label><div class="relative" data-node-hydration="13"><div class="pointer-events-none absolute inset-y-0 left-0 flex items-center pl-3"><svg viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="h-5 w-5 text-gray-400"><path fill-rule="evenodd" clip-rule="evenodd" d="M9 3.5a5.5 5.5 0 100 11 5.5 5.5 0 000-11zM2 9a7 7 0 1112.452 4.391l3.328 3.329a.75.75 0 11-1.06 1.06l-3.329-3.328A7 7 0 012 9z"></path></svg></div><button name="search" class="block w-full rounded-md border-0 bg-white py-1.5 pl-10 pr-3 text-gray-400 ring-1 ring-inset ring-gray-300 focus:ring-2 focus:ring-inset focus:ring-indigo-600 sm:text-sm sm:leading-6" id="search" data-node-hydration="14,click:1"><div class="h-full my-auto flex flex-row align-middle justify-between"><span class="md:pl-2">Search</span><div class="ml-3 flex-none font-semibold text-gray-500"><kbd class="font-sans">CTRL + /</kbd></div></div></button></div></div></div><div class="flex items-center lg:hidden"><button aria-expanded="false" type="button" aria-controls="mobile-menu" class="relative inline-flex items-center justify-center rounded-md p-2 text-gray-400 hover:bg-gray-100 hover:text-gray-500 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-indigo-500" data-node-hydration="15,click:1"><span class="absolute -inset-0.5"></span><span class="sr-only">Open main menu</span><svg stroke="currentColor" fill="none" stroke-width="1.5" viewBox="0 0 24 24" aria-hidden="true" class="block h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg><svg viewBox="0 0 24 24" stroke="currentColor" stroke-width="1.5" aria-hidden="true" fill="none" class="hidden h-6 w-6"><path stroke-linejoin="round" d="M6 18L18 6M6 6l12 12" stroke-linecap="round"></path></svg></button></div></div></div><div class="lg:hidden hidden" id="mobile-menu" data-node-hydration="16"><div class="space-y-1 pb-3 pt-2"><a href="/" class="block border-l-4 border-transparent py-2 pl-3 pr-4 text-base font-medium text-gray-600 hover:border-gray-300 hover:bg-gray-50 hover:text-gray-800" role="menuitem" tabindex="-1" data-node-hydration="17,click:1"><!--node-id18-->Home<!--#--></a><a href="/docs/" class="block border-l-4 border-transparent py-2 pl-3 pr-4 text-base font-medium text-gray-600 hover:border-gray-300 hover:bg-gray-50 hover:text-gray-800" role="menuitem" tabindex="-1" data-node-hydration="19,click:1"><!--node-id20-->Docs<!--#--></a></div></div></nav><div class="w-full pt-12 backdrop-blur-lg bg-white/75" style="min-height:100vh;" data-node-hydration="21"><div class="max-w-screen-2xl flex flex-row justify-between mx-auto"><!--placeholder22--><section class="body-font overflow-hidden mx-auto container pt-12 pb-12 md:w-2/3" data-node-hydration="23"><div class="-my-8"><div class="w-full mb-20 flex-wrap list-none rounded-md"><article class="markdown-body pt-1"><h1 id="introducing-kalosm-04-a-local-first-ai-meta-framework-for-rust" data-node-hydration="24"><a href="/blog/kalosm_0_4#introducing-kalosm-04-a-local-first-ai-meta-framework-for-rust" class="header" data-node-hydration="25,click:1"><!--node-id26-->Introducing Kalosm 0.4: A Local-First AI Meta-Framework for Rust<!--#--></a></h1><p data-node-hydration="27">Kalosm is a simple, local first interface for pre-trained language, audio, and image models written in Rust. It is built on top of the <a href="https://github.com/huggingface/candle" rel="noopener noreferrer" data-node-hydration="28,click:1"><!--node-id29-->candle<!--#--></a> library for ML inference. Kalosm makes it easy to run quantized models on your local machine accelerated by cuda or metal. Whether you’re building a chatbot, a transcription tool, or an image processing application, Kalosm 0.4 brings major improvements to help you get started quickly and efficiently.</p><p data-node-hydration="30">In this release, we’re excited to announce:</p><ul data-node-hydration="31"><li><strong>A Simplified API</strong> for language models, tasks, and chat sessions</li><li><strong>Extended support for remote models</strong> via OpenAI and Anthropic adapters with structured generation</li><li><strong>Single-file model loading</strong> that consolidates model weights, tokenizer, and chat template metadata into one GGUF file</li><li><strong>Improved whisper transcription</strong> with word-level timestamps using dynamic time warping</li></ul><hr data-node-hydration="32"/><h2 id="simplified-llm-api" data-node-hydration="33"><a href="/blog/kalosm_0_4#simplified-llm-api" class="header" data-node-hydration="34,click:1"><!--node-id35-->Simplified LLM API<!--#--></a></h2><p data-node-hydration="36">One of the biggest improvements in Kalosm 0.4 is the streamlined API for interacting with local large language models. Developers can now configure chat sessions and tasks with a series of builder methods until the first message is added. This flexibility simplifies the code and makes your applications easier to maintain.</p><p data-node-hydration="37">For example, here’s how you can set up a chat session with a system prompt:</p><div class="border overflow-hidden rounded-md border-gray-300 dark:border-gray-700 mb-8" style="position:relative;" data-node-hydration="38"><button class="flex flex-row items-center gap-1 hover:text-blue-600" onclick="navigator.clipboard.writeText(this.parentNode.lastChild.innerText);" style="position:absolute;top:0;right:0;" data-node-hydration="39,click:1"><!--placeholder40--><span><svg width="24" height="24" stroke-width="1.5" fill="none" stroke="currentColor" data-node-hydration="41"><path d="M8 16c0 1.886 0 2.828.586 3.414C9.172 20 10.114 20 12 20h4c1.886 0 2.828 0 3.414-.586C20 18.828 20 17.886 20 16v-4c0-1.886 0-2.828-.586-3.414C18.828 8 17.886 8 16 8m-8 8h4c1.886 0 2.828 0 3.414-.586C16 14.828 16 13.886 16 12V8m-8 8c-1.886 0-2.828 0-3.414-.586C4 14.828 4 13.886 4 12V8c0-1.886 0-2.828.586-3.414C5.172 4 6.114 4 8 4h4c1.886 0 2.828 0 3.414.586C16 5.172 16 6.114 16 8"></path></svg></span></button><div class="overflow-x-scroll bg-[#0d0d0d]" data-node-hydration="42"><pre style="background-color:#0d0d0d;">
<span style="color:#f92672;">use </span><span style="color:#f8f8f2;">kalosm::language::</span><span style="color:#f92672;">*</span><span style="color:#f8f8f2;">;
</span><span style="color:#f8f8f2;">
</span><span style="color:#8c8c8c;">// Create a new chat-capable Llama model
</span><span style="font-style:italic;color:#66d9ef;">let</span><span style="color:#f8f8f2;"> model </span><span style="color:#f92672;">= </span><span style="color:#f8f8f2;">Llama::new_chat().await</span><span style="color:#f92672;">?</span><span style="color:#f8f8f2;">;
</span><span style="color:#f8f8f2;">
</span><span style="color:#8c8c8c;">// Configure a chat session with a system prompt
</span><span style="font-style:italic;color:#66d9ef;">let </span><span style="color:#f92672;">mut</span><span style="color:#f8f8f2;"> chat </span><span style="color:#f92672;">=</span><span style="color:#f8f8f2;"> model
</span><span style="color:#f8f8f2;">    .</span><span style="color:#66d9ef;">chat</span><span style="color:#f8f8f2;">()
</span><span style="color:#f8f8f2;">    .</span><span style="color:#66d9ef;">with_system_prompt</span><span style="color:#f8f8f2;">(</span><span style="color:#ffee99;">&quot;The assistant will act like a pirate&quot;</span><span style="color:#f8f8f2;">);
</span><span style="color:#f8f8f2;">
</span><span style="color:#f92672;">loop </span><span style="color:#f8f8f2;">{
</span><span style="color:#f8f8f2;">    </span><span style="color:#8c8c8c;">// Get user input and stream the model’s response to stdout
</span><span style="color:#f8f8f2;">    </span><span style="color:#66d9ef;">chat</span><span style="color:#f8f8f2;">(</span><span style="color:#f92672;">&amp;</span><span style="color:#66d9ef;">prompt_input</span><span style="color:#f8f8f2;">(</span><span style="color:#ffee99;">&quot;</span><span style="color:#ff80f4;">\n</span><span style="color:#ffee99;">&gt; &quot;</span><span style="color:#f8f8f2;">)</span><span style="color:#f92672;">?</span><span style="color:#f8f8f2;">)
</span><span style="color:#f8f8f2;">        .</span><span style="color:#66d9ef;">to_std_out</span><span style="color:#f8f8f2;">()
</span><span style="color:#f8f8f2;">        .await</span><span style="color:#f92672;">?</span><span style="color:#f8f8f2;">;
</span><span style="color:#f8f8f2;">}</span></pre>
</div></div><p data-node-hydration="43">Similarly, the task API now supports automatic constraint inference. By deriving parsing and schema traits on your data types, you can easily generate concrete types from your language model:</p><div class="border overflow-hidden rounded-md border-gray-300 dark:border-gray-700 mb-8" style="position:relative;" data-node-hydration="44"><button class="flex flex-row items-center gap-1 hover:text-blue-600" onclick="navigator.clipboard.writeText(this.parentNode.lastChild.innerText);" style="position:absolute;top:0;right:0;" data-node-hydration="45,click:1"><!--placeholder46--><span><svg width="24" height="24" stroke-width="1.5" fill="none" stroke="currentColor" data-node-hydration="47"><path d="M8 16c0 1.886 0 2.828.586 3.414C9.172 20 10.114 20 12 20h4c1.886 0 2.828 0 3.414-.586C20 18.828 20 17.886 20 16v-4c0-1.886 0-2.828-.586-3.414C18.828 8 17.886 8 16 8m-8 8h4c1.886 0 2.828 0 3.414-.586C16 14.828 16 13.886 16 12V8m-8 8c-1.886 0-2.828 0-3.414-.586C4 14.828 4 13.886 4 12V8c0-1.886 0-2.828.586-3.414C5.172 4 6.114 4 8 4h4c1.886 0 2.828 0 3.414.586C16 5.172 16 6.114 16 8"></path></svg></span></button><div class="overflow-x-scroll bg-[#0d0d0d]" data-node-hydration="48"><pre style="background-color:#0d0d0d;">
<span style="color:#f92672;">use </span><span style="color:#f8f8f2;">kalosm::language::</span><span style="color:#f92672;">*</span><span style="color:#f8f8f2;">;
</span><span style="color:#f8f8f2;">
</span><span style="color:#8c8c8c;">/// A fictional account holder
</span><span style="color:#f8f8f2;">#[derive(Schema, Parse, Clone, Debug)]
</span><span style="font-style:italic;color:#66d9ef;">struct </span><span style="color:#f8f8f2;">Account {
</span><span style="color:#f8f8f2;">    </span><span style="color:#8c8c8c;">/// A brief summary of the account holder
</span><span style="color:#f8f8f2;">    #[parse(pattern = r&quot;[a-zA-Z,.?!\d ]{1,80}</span><span style="color:#ffee99;">&quot;)]
</span><span style="color:#ffee99;">    summary: String,
</span><span style="color:#ffee99;">    /// The account holder’s name (full name or pseudonym)
</span><span style="color:#ffee99;">    #[parse(pattern = &quot;</span><span style="color:#f8f8f2;">[a</span><span style="color:#f92672;">-</span><span style="color:#f8f8f2;">zA</span><span style="color:#f92672;">-</span><span style="color:#f8f8f2;">Z ]{</span><span style="color:#ff80f4;">1</span><span style="color:#f8f8f2;">,</span><span style="color:#ff80f4;">20</span><span style="color:#f8f8f2;">}</span><span style="color:#ffee99;">&quot;)]
</span><span style="color:#ffee99;">    name: String,
</span><span style="color:#ffee99;">    /// The account holder’s age
</span><span style="color:#ffee99;">    #[parse(range = 1..=100)]
</span><span style="color:#ffee99;">    age: u8,
</span><span style="color:#ffee99;">}
</span><span style="color:#ffee99;">
</span><span style="color:#ffee99;">// Create a small reasoning-focused model
</span><span style="color:#ffee99;">let llm = Llama::phi_3().await?;
</span><span style="color:#ffee99;">
</span><span style="color:#ffee99;">// Create a task for generating accounts, with automatic type-based constraint inference
</span><span style="color:#ffee99;">let create_account = llm
</span><span style="color:#ffee99;">    .task(&quot;</span><span style="color:#f8f8f2;">You generate accounts based on a description of the account holde</span><span style="font-style:italic;color:#66d9ef;">r</span><span style="color:#ffee99;">&quot;)
</span><span style="color:#ffee99;">    .typed();
</span><span style="color:#ffee99;">
</span><span style="color:#ffee99;">// Generate an account from a natural language prompt
</span><span style="color:#ffee99;">let account: Account = create_account(
</span><span style="color:#ffee99;">    &quot;</span><span style="color:#f8f8f2;">Candice is the </span><span style="color:#ff80f4;">CEO</span><span style="color:#f8f8f2;"> of a fortune </span><span style="color:#ff80f4;">500</span><span style="color:#f8f8f2;"> company. She is </span><span style="color:#ff80f4;">30</span><span style="color:#f8f8f2;"> years old.</span><span style="color:#ffee99;">&quot;
</span><span style="color:#ffee99;">).await?;
</span><span style="color:#ffee99;">
</span><span style="color:#ffee99;">println!(&quot;</span><span style="color:#f8f8f2;">Generated Account: {account:</span><span style="color:#f92672;">?</span><span style="color:#f8f8f2;">}</span><span style="color:#ffee99;">&quot;);</span></pre>
</div></div><hr data-node-hydration="49"/><h2 id="openai-and-anthropic-support-with-structured-generation" data-node-hydration="50"><a href="/blog/kalosm_0_4#openai-and-anthropic-support-with-structured-generation" class="header" data-node-hydration="51,click:1"><!--node-id52-->OpenAI and Anthropic Support with Structured Generation<!--#--></a></h2><p data-node-hydration="53">Kalosm 0.4 extends its support for remote models by integrating with the OpenAI and Anthropic chat APIs. Now you can use Kalosm’s structured generation even when working with remote chat models. Unfortunately Anthropic currently doesn&#x27;t support structured generation, so support for tasks with Anthropic models is currently limited.</p><p data-node-hydration="54">To work with OpenAI models, enable the  <code>openai</code> feature and set your API key via the  <code>OPENAI_API_KEY</code> environment variable. Then create a model adapter:</p><div class="border overflow-hidden rounded-md border-gray-300 dark:border-gray-700 mb-8" style="position:relative;" data-node-hydration="55"><button class="flex flex-row items-center gap-1 hover:text-blue-600" onclick="navigator.clipboard.writeText(this.parentNode.lastChild.innerText);" style="position:absolute;top:0;right:0;" data-node-hydration="56,click:1"><!--placeholder57--><span><svg width="24" height="24" stroke-width="1.5" fill="none" stroke="currentColor" data-node-hydration="58"><path d="M8 16c0 1.886 0 2.828.586 3.414C9.172 20 10.114 20 12 20h4c1.886 0 2.828 0 3.414-.586C20 18.828 20 17.886 20 16v-4c0-1.886 0-2.828-.586-3.414C18.828 8 17.886 8 16 8m-8 8h4c1.886 0 2.828 0 3.414-.586C16 14.828 16 13.886 16 12V8m-8 8c-1.886 0-2.828 0-3.414-.586C4 14.828 4 13.886 4 12V8c0-1.886 0-2.828.586-3.414C5.172 4 6.114 4 8 4h4c1.886 0 2.828 0 3.414.586C16 5.172 16 6.114 16 8"></path></svg></span></button><div class="overflow-x-scroll bg-[#0d0d0d]" data-node-hydration="59"><pre style="background-color:#0d0d0d;">
<span style="color:#8c8c8c;">// Create a chat model adapter for OpenAI-compatible models
</span><span style="font-style:italic;color:#66d9ef;">let</span><span style="color:#f8f8f2;"> llm </span><span style="color:#f92672;">= </span><span style="color:#f8f8f2;">OpenAICompatibleChatModel::builder()
</span><span style="color:#f8f8f2;">    .</span><span style="color:#66d9ef;">with_gpt_4o_mini</span><span style="color:#f8f8f2;">()
</span><span style="color:#f8f8f2;">    .</span><span style="color:#66d9ef;">build</span><span style="color:#f8f8f2;">();
</span><span style="color:#f8f8f2;">
</span><span style="color:#8c8c8c;">// Use the adapter just like any other chat model
</span><span style="font-style:italic;color:#66d9ef;">let</span><span style="color:#f8f8f2;"> generate_character </span><span style="color:#f92672;">=</span><span style="color:#f8f8f2;"> llm
</span><span style="color:#f8f8f2;">    .</span><span style="color:#66d9ef;">task</span><span style="color:#f8f8f2;">(</span><span style="color:#ffee99;">&quot;You generate accounts based on a description of the account holder&quot;</span><span style="color:#f8f8f2;">)
</span><span style="color:#f8f8f2;">    .</span><span style="color:#66d9ef;">typed</span><span style="color:#f8f8f2;">();
</span><span style="color:#f8f8f2;">
</span><span style="font-style:italic;color:#66d9ef;">let</span><span style="color:#f8f8f2;"> account: Account </span><span style="color:#f92672;">= </span><span style="color:#66d9ef;">generate_character</span><span style="color:#f8f8f2;">(
</span><span style="color:#f8f8f2;">    </span><span style="color:#ffee99;">&quot;Candice is the CEO of a fortune 500 company. She is 30 years old.&quot;
</span><span style="color:#f8f8f2;">).await</span><span style="color:#f92672;">?</span><span style="color:#f8f8f2;">;
</span><span style="color:#f8f8f2;">println!(</span><span style="color:#ffee99;">&quot;Generated Account: </span><span style="color:#ff80f4;">{account:?}</span><span style="color:#ffee99;">&quot;</span><span style="color:#f8f8f2;">);</span></pre>
</div></div><p data-node-hydration="60">For Anthropic models, enable the  <code>anthropic</code> feature and set your  <code>ANTHROPIC_API_KEY</code>:</p><div class="border overflow-hidden rounded-md border-gray-300 dark:border-gray-700 mb-8" style="position:relative;" data-node-hydration="61"><button class="flex flex-row items-center gap-1 hover:text-blue-600" onclick="navigator.clipboard.writeText(this.parentNode.lastChild.innerText);" style="position:absolute;top:0;right:0;" data-node-hydration="62,click:1"><!--placeholder63--><span><svg width="24" height="24" stroke-width="1.5" fill="none" stroke="currentColor" data-node-hydration="64"><path d="M8 16c0 1.886 0 2.828.586 3.414C9.172 20 10.114 20 12 20h4c1.886 0 2.828 0 3.414-.586C20 18.828 20 17.886 20 16v-4c0-1.886 0-2.828-.586-3.414C18.828 8 17.886 8 16 8m-8 8h4c1.886 0 2.828 0 3.414-.586C16 14.828 16 13.886 16 12V8m-8 8c-1.886 0-2.828 0-3.414-.586C4 14.828 4 13.886 4 12V8c0-1.886 0-2.828.586-3.414C5.172 4 6.114 4 8 4h4c1.886 0 2.828 0 3.414.586C16 5.172 16 6.114 16 8"></path></svg></span></button><div class="overflow-x-scroll bg-[#0d0d0d]" data-node-hydration="65"><pre style="background-color:#0d0d0d;">
<span style="color:#8c8c8c;">// Create a chat model adapter for Anthropic-compatible models
</span><span style="font-style:italic;color:#66d9ef;">let</span><span style="color:#f8f8f2;"> llm </span><span style="color:#f92672;">= </span><span style="color:#f8f8f2;">AnthropicCompatibleChatModel::builder()
</span><span style="color:#f8f8f2;">    .</span><span style="color:#66d9ef;">with_claude_3_5_haiku</span><span style="color:#f8f8f2;">()
</span><span style="color:#f8f8f2;">    .</span><span style="color:#66d9ef;">build</span><span style="color:#f8f8f2;">();
</span><span style="color:#f8f8f2;">
</span><span style="color:#8c8c8c;">// Start a chat session with a custom system prompt
</span><span style="font-style:italic;color:#66d9ef;">let </span><span style="color:#f92672;">mut</span><span style="color:#f8f8f2;"> chat </span><span style="color:#f92672;">=</span><span style="color:#f8f8f2;"> llm
</span><span style="color:#f8f8f2;">    .</span><span style="color:#66d9ef;">chat</span><span style="color:#f8f8f2;">()
</span><span style="color:#f8f8f2;">    .</span><span style="color:#66d9ef;">with_system_prompt</span><span style="color:#f8f8f2;">(</span><span style="color:#ffee99;">&quot;The assistant will act like a pirate&quot;</span><span style="color:#f8f8f2;">);
</span><span style="color:#f8f8f2;">
</span><span style="color:#f92672;">loop </span><span style="color:#f8f8f2;">{
</span><span style="color:#f8f8f2;">    </span><span style="color:#66d9ef;">chat</span><span style="color:#f8f8f2;">(</span><span style="color:#f92672;">&amp;</span><span style="color:#66d9ef;">prompt_input</span><span style="color:#f8f8f2;">(</span><span style="color:#ffee99;">&quot;</span><span style="color:#ff80f4;">\n</span><span style="color:#ffee99;">&gt; &quot;</span><span style="color:#f8f8f2;">)</span><span style="color:#f92672;">?</span><span style="color:#f8f8f2;">)
</span><span style="color:#f8f8f2;">        .</span><span style="color:#66d9ef;">to_std_out</span><span style="color:#f8f8f2;">()
</span><span style="color:#f8f8f2;">        .await</span><span style="color:#f92672;">?</span><span style="color:#f8f8f2;">;
</span><span style="color:#f8f8f2;">}</span></pre>
</div></div><p data-node-hydration="66">These integrations allow you to combine local inference with remote model capabilities and structured generation features, making it easier to build hybrid AI applications where local models may not be enough.</p><hr data-node-hydration="67"/><h2 id="single-file-llm-loading" data-node-hydration="68"><a href="/blog/kalosm_0_4#single-file-llm-loading" class="header" data-node-hydration="69,click:1"><!--node-id70-->Single-File LLM Loading<!--#--></a></h2><p data-node-hydration="71">Previously, loading a custom model required managing separate files for the tokenizer, chat template, and model weights. With version 0.4, Kalosm supports loading all model metadata from a single GGUF file. This improvement simplifies model distribution and deployment.</p><p data-node-hydration="72">To load a custom model, you can now just point to a local or huggingface GGUF file:</p><div class="border overflow-hidden rounded-md border-gray-300 dark:border-gray-700 mb-8" style="position:relative;" data-node-hydration="73"><button class="flex flex-row items-center gap-1 hover:text-blue-600" onclick="navigator.clipboard.writeText(this.parentNode.lastChild.innerText);" style="position:absolute;top:0;right:0;" data-node-hydration="74,click:1"><!--placeholder75--><span><svg width="24" height="24" stroke-width="1.5" fill="none" stroke="currentColor" data-node-hydration="76"><path d="M8 16c0 1.886 0 2.828.586 3.414C9.172 20 10.114 20 12 20h4c1.886 0 2.828 0 3.414-.586C20 18.828 20 17.886 20 16v-4c0-1.886 0-2.828-.586-3.414C18.828 8 17.886 8 16 8m-8 8h4c1.886 0 2.828 0 3.414-.586C16 14.828 16 13.886 16 12V8m-8 8c-1.886 0-2.828 0-3.414-.586C4 14.828 4 13.886 4 12V8c0-1.886 0-2.828.586-3.414C5.172 4 6.114 4 8 4h4c1.886 0 2.828 0 3.414.586C16 5.172 16 6.114 16 8"></path></svg></span></button><div class="overflow-x-scroll bg-[#0d0d0d]" data-node-hydration="77"><pre style="background-color:#0d0d0d;">
<span style="font-style:italic;color:#66d9ef;">let</span><span style="color:#f8f8f2;"> model </span><span style="color:#f92672;">= </span><span style="color:#f8f8f2;">Llama::builder()
</span><span style="color:#f8f8f2;">    </span><span style="color:#8c8c8c;">// Specify a custom model source using a GGUF file
</span><span style="color:#f8f8f2;">    .</span><span style="color:#66d9ef;">with_source</span><span style="color:#f8f8f2;">(LlamaSource::new(
</span><span style="color:#f8f8f2;">        FileSource::HuggingFace {
</span><span style="color:#f8f8f2;">            model_id: </span><span style="color:#ffee99;">&quot;QuantFactory/SmolLM-1.7B-Instruct-GGUF&quot;</span><span style="color:#f8f8f2;">.</span><span style="color:#66d9ef;">to_string</span><span style="color:#f8f8f2;">(),
</span><span style="color:#f8f8f2;">            revision: </span><span style="color:#ffee99;">&quot;main&quot;</span><span style="color:#f8f8f2;">.</span><span style="color:#66d9ef;">to_string</span><span style="color:#f8f8f2;">(),
</span><span style="color:#f8f8f2;">            file: </span><span style="color:#ffee99;">&quot;SmolLM-1.7B-Instruct.Q4_K_M.gguf&quot;</span><span style="color:#f8f8f2;">.</span><span style="color:#66d9ef;">to_string</span><span style="color:#f8f8f2;">(),
</span><span style="color:#f8f8f2;">        },
</span><span style="color:#f8f8f2;">    ))
</span><span style="color:#f8f8f2;">    .</span><span style="color:#66d9ef;">build</span><span style="color:#f8f8f2;">()
</span><span style="color:#f8f8f2;">    .await</span><span style="color:#f92672;">?</span><span style="color:#f8f8f2;">;
</span><span style="color:#f8f8f2;">
</span><span style="font-style:italic;color:#66d9ef;">let </span><span style="color:#f92672;">mut</span><span style="color:#f8f8f2;"> chat </span><span style="color:#f92672;">=</span><span style="color:#f8f8f2;"> model
</span><span style="color:#f8f8f2;">    .</span><span style="color:#66d9ef;">chat</span><span style="color:#f8f8f2;">()
</span><span style="color:#f8f8f2;">    .</span><span style="color:#66d9ef;">with_system_prompt</span><span style="color:#f8f8f2;">(</span><span style="color:#ffee99;">&quot;The assistant will act like a pirate&quot;</span><span style="color:#f8f8f2;">);
</span><span style="color:#f8f8f2;">
</span><span style="color:#f92672;">loop </span><span style="color:#f8f8f2;">{
</span><span style="color:#f8f8f2;">    </span><span style="color:#66d9ef;">chat</span><span style="color:#f8f8f2;">(</span><span style="color:#f92672;">&amp;</span><span style="color:#66d9ef;">prompt_input</span><span style="color:#f8f8f2;">(</span><span style="color:#ffee99;">&quot;</span><span style="color:#ff80f4;">\n</span><span style="color:#ffee99;">&gt; &quot;</span><span style="color:#f8f8f2;">)</span><span style="color:#f92672;">?</span><span style="color:#f8f8f2;">)
</span><span style="color:#f8f8f2;">        .</span><span style="color:#66d9ef;">to_std_out</span><span style="color:#f8f8f2;">()
</span><span style="color:#f8f8f2;">        .await</span><span style="color:#f92672;">?</span><span style="color:#f8f8f2;">;
</span><span style="color:#f8f8f2;">}</span></pre>
</div></div><p data-node-hydration="78">This unified approach to model loading makes it much easier to switch between different models and update to new models over time.</p><hr data-node-hydration="79"/><h2 id="timestamped-transcription-with-whisper" data-node-hydration="80"><a href="/blog/kalosm_0_4#timestamped-transcription-with-whisper" class="header" data-node-hydration="81,click:1"><!--node-id82-->Timestamped Transcription with Whisper<!--#--></a></h2><p data-node-hydration="83">Kalosm 0.4 also brings improvements to audio transcription. The new Whisper API supports word-level timestamps, which can be critical for applications that require precise alignment between audio and text.</p><p data-node-hydration="84">Below is an example that demonstrates how to transcribe audio and print each segment along with its corresponding timestamps:</p><div class="border overflow-hidden rounded-md border-gray-300 dark:border-gray-700 mb-8" style="position:relative;" data-node-hydration="85"><button class="flex flex-row items-center gap-1 hover:text-blue-600" onclick="navigator.clipboard.writeText(this.parentNode.lastChild.innerText);" style="position:absolute;top:0;right:0;" data-node-hydration="86,click:1"><!--placeholder87--><span><svg width="24" height="24" stroke-width="1.5" fill="none" stroke="currentColor" data-node-hydration="88"><path d="M8 16c0 1.886 0 2.828.586 3.414C9.172 20 10.114 20 12 20h4c1.886 0 2.828 0 3.414-.586C20 18.828 20 17.886 20 16v-4c0-1.886 0-2.828-.586-3.414C18.828 8 17.886 8 16 8m-8 8h4c1.886 0 2.828 0 3.414-.586C16 14.828 16 13.886 16 12V8m-8 8c-1.886 0-2.828 0-3.414-.586C4 14.828 4 13.886 4 12V8c0-1.886 0-2.828.586-3.414C5.172 4 6.114 4 8 4h4c1.886 0 2.828 0 3.414.586C16 5.172 16 6.114 16 8"></path></svg></span></button><div class="overflow-x-scroll bg-[#0d0d0d]" data-node-hydration="89"><pre style="background-color:#0d0d0d;">
<span style="color:#8c8c8c;">// Build a new Whisper model using a quantized variant
</span><span style="font-style:italic;color:#66d9ef;">let</span><span style="color:#f8f8f2;"> model </span><span style="color:#f92672;">= </span><span style="color:#f8f8f2;">WhisperBuilder::default()
</span><span style="color:#f8f8f2;">    .</span><span style="color:#66d9ef;">with_source</span><span style="color:#f8f8f2;">(WhisperSource::QuantizedLargeV3Turbo)
</span><span style="color:#f8f8f2;">    .</span><span style="color:#66d9ef;">build</span><span style="color:#f8f8f2;">()
</span><span style="color:#f8f8f2;">    .await</span><span style="color:#f92672;">?</span><span style="color:#f8f8f2;">;
</span><span style="color:#f8f8f2;">
</span><span style="color:#8c8c8c;">// Open and decode an audio file
</span><span style="font-style:italic;color:#66d9ef;">let</span><span style="color:#f8f8f2;"> file </span><span style="color:#f92672;">= </span><span style="color:#f8f8f2;">BufReader::new(File::open(</span><span style="color:#ffee99;">&quot;./samples_jfk.wav&quot;</span><span style="color:#f8f8f2;">)</span><span style="color:#f92672;">?</span><span style="color:#f8f8f2;">);
</span><span style="font-style:italic;color:#66d9ef;">let</span><span style="color:#f8f8f2;"> audio </span><span style="color:#f92672;">= </span><span style="color:#f8f8f2;">Decoder::new(file)</span><span style="color:#f92672;">?</span><span style="color:#f8f8f2;">;
</span><span style="color:#f8f8f2;">
</span><span style="color:#8c8c8c;">// Transcribe the audio with timestamping enabled
</span><span style="font-style:italic;color:#66d9ef;">let </span><span style="color:#f92672;">mut</span><span style="color:#f8f8f2;"> text </span><span style="color:#f92672;">=</span><span style="color:#f8f8f2;"> model.</span><span style="color:#66d9ef;">transcribe</span><span style="color:#f8f8f2;">(audio).</span><span style="color:#66d9ef;">timestamped</span><span style="color:#f8f8f2;">();
</span><span style="color:#f8f8f2;">
</span><span style="color:#8c8c8c;">// Print each transcribed segment with its start and end times
</span><span style="color:#f92672;">while </span><span style="font-style:italic;color:#66d9ef;">let Some</span><span style="color:#f8f8f2;">(segment) </span><span style="color:#f92672;">=</span><span style="color:#f8f8f2;"> text.</span><span style="color:#66d9ef;">next</span><span style="color:#f8f8f2;">().await {
</span><span style="color:#f8f8f2;">    </span><span style="color:#f92672;">for</span><span style="color:#f8f8f2;"> chunk </span><span style="color:#f92672;">in</span><span style="color:#f8f8f2;"> segment.</span><span style="color:#66d9ef;">chunks</span><span style="color:#f8f8f2;">() {
</span><span style="color:#f8f8f2;">        </span><span style="font-style:italic;color:#66d9ef;">let</span><span style="color:#f8f8f2;"> timestamp </span><span style="color:#f92672;">=</span><span style="color:#f8f8f2;"> chunk.</span><span style="color:#66d9ef;">timestamp</span><span style="color:#f8f8f2;">().</span><span style="color:#66d9ef;">unwrap</span><span style="color:#f8f8f2;">();
</span><span style="color:#f8f8f2;">        println!(</span><span style="color:#ffee99;">&quot;</span><span style="color:#ff80f4;">{:0.2}</span><span style="color:#ffee99;">..</span><span style="color:#ff80f4;">{:0.2}</span><span style="color:#ffee99;">: </span><span style="color:#ff80f4;">{}</span><span style="color:#ffee99;">&quot;</span><span style="color:#f8f8f2;">, timestamp.start, timestamp.end, chunk);
</span><span style="color:#f8f8f2;">    }
</span><span style="color:#f8f8f2;">}</span></pre>
</div></div><p data-node-hydration="90">This level of granularity in transcription is particularly useful for applications like video captioning, meeting transcription, or any context where knowing the exact timing of spoken words matters.</p><hr data-node-hydration="91"/><h2 id="community-and-support" data-node-hydration="92"><a href="/blog/kalosm_0_4#community-and-support" class="header" data-node-hydration="93,click:1"><!--node-id94-->Community and Support<!--#--></a></h2><p data-node-hydration="95">Kalosm is built by a passionate community dedicated to making local AI inference accessible and efficient. For help, discussion, or to share feedback, join the <a href="https://discord.gg/dQdmhuB8q5" rel="noopener noreferrer" data-node-hydration="96,click:1"><!--node-id97-->Kalosm Discord community<!--#--></a>. Whether you’re troubleshooting, looking to contribute, or simply curious about the latest updates, our Discord is the best place to connect with fellow developers and the Kalosm team.</p><hr data-node-hydration="98"/><h2 id="conclusion" data-node-hydration="99"><a href="/blog/kalosm_0_4#conclusion" class="header" data-node-hydration="100,click:1"><!--node-id101-->Conclusion<!--#--></a></h2><p data-node-hydration="102">Kalosm 0.4 brings a range of enhancements—from simplified APIs and unified model loading to extended remote support and precise transcription capabilities—that we hope will make your development process smoother and more enjoyable. We&#x27;re truly excited to see the creative applications and projects you build with these tools. </p><p data-node-hydration="103">Happy coding!</p></article></div></div></section><div class="hidden xl:sticky xl:top-[4.75rem] xl:-mr-6 xl:block xl:h-[calc(100vh-4.75rem)] xl:flex-none xl:overflow-y-auto xl:py-16 xl:pr-6" data-node-hydration="104"><nav aria-labelledby="on-this-page-title" class="w-56"><h2 id="on-this-page-title" class="font-display text-sm font-medium text-slate-900" data-node-hydration="105">On this page</h2><ol role="list" class="mt-4 space-y-3 text-sm" data-node-hydration="106"><li class="pb-2 pl-2" data-node-hydration="107"><h3><a class="font-normal text-slate-500 hover:text-slate-700" href="#introducing-kalosm-0.4:-a-local-first-ai-meta-framework-for-rust" data-node-hydration="108"><!--node-id109-->Introducing Kalosm 0.4: A Local-First AI Meta-Framework for Rust<!--#--></a></h3></li><li class="pb-2 pl-4" data-node-hydration="110"><h3><a class="font-normal text-slate-500 hover:text-slate-700" href="#simplified-llm-api" data-node-hydration="111"><!--node-id112-->Simplified LLM API<!--#--></a></h3></li><li class="pb-2 pl-4" data-node-hydration="113"><h3><a class="font-normal text-slate-500 hover:text-slate-700" href="#openai-and-anthropic-support-with-structured-generation" data-node-hydration="114"><!--node-id115-->OpenAI and Anthropic Support with Structured Generation<!--#--></a></h3></li><li class="pb-2 pl-4" data-node-hydration="116"><h3><a class="font-normal text-slate-500 hover:text-slate-700" href="#single-file-llm-loading" data-node-hydration="117"><!--node-id118-->Single-File LLM Loading<!--#--></a></h3></li><li class="pb-2 pl-4" data-node-hydration="119"><h3><a class="font-normal text-slate-500 hover:text-slate-700" href="#timestamped-transcription-with-whisper" data-node-hydration="120"><!--node-id121-->Timestamped Transcription with Whisper<!--#--></a></h3></li><li class="pb-2 pl-4" data-node-hydration="122"><h3><a class="font-normal text-slate-500 hover:text-slate-700" href="#community-and-support" data-node-hydration="123"><!--node-id124-->Community and Support<!--#--></a></h3></li><li class="pb-2 pl-4" data-node-hydration="125"><h3><a class="font-normal text-slate-500 hover:text-slate-700" href="#conclusion" data-node-hydration="126"><!--node-id127-->Conclusion<!--#--></a></h3></li></ol></nav></div></div></div><footer class="bg-white" data-node-hydration="128"><div class="mx-auto max-w-7xl overflow-hidden px-6 py-20 sm:py-24 lg:px-8"><nav aria-label="Footer" class="-mb-6 columns-2 sm:flex sm:justify-center sm:space-x-12"><div class="pb-6" data-node-hydration="129"><a href="/" class="text-sm leading-6 text-gray-600 hover:text-gray-900" data-node-hydration="130,click:1"><!--node-id131-->About<!--#--></a></div><div class="pb-6" data-node-hydration="132"><a href="/docs/" class="text-sm leading-6 text-gray-600 hover:text-gray-900" data-node-hydration="133,click:1"><!--node-id134-->Docs<!--#--></a></div><div class="pb-6" data-node-hydration="135"><a href="/blog/" class="text-sm leading-6 text-gray-600 hover:text-gray-900" data-node-hydration="136,click:1"><!--node-id137-->Blog<!--#--></a></div></nav><div class="mt-10 flex justify-center space-x-10"><a href="https://github.com/floneum/floneum/tree/main/floneum/floneum" class="text-gray-400 hover:text-gray-500" rel="noopener noreferrer" data-node-hydration="138,click:1"><span class="sr-only" data-node-hydration="139">Github</span><svg class="h-6 w-6" viewBox="0 0 24 24" aria-hidden="true" data-node-hydration="140"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z" fill="currentColor" fill-rule="nonzero"></path></svg></a><a href="https://discord.gg/dQdmhuB8q5" class="text-gray-400 hover:text-gray-500" rel="noopener noreferrer" data-node-hydration="141,click:1"><span class="sr-only" data-node-hydration="142">Discord</span><svg class="h-6 w-6" viewBox="0 -28.5 256 256" preserveAspectRatio="xMidYMid" aria-hidden="true" data-node-hydration="143"><path d="M216.856339,16.5966031 C200.285002,8.84328665 182.566144,3.2084988 164.041564,0 C161.766523,4.11318106 159.108624,9.64549908 157.276099,14.0464379 C137.583995,11.0849896 118.072967,11.0849896 98.7430163,14.0464379 C96.9108417,9.64549908 94.1925838,4.11318106 91.8971895,0 C73.3526068,3.2084988 55.6133949,8.86399117 39.0420583,16.6376612 C5.61752293,67.146514 -3.4433191,116.400813 1.08711069,164.955721 C23.2560196,181.510915 44.7403634,191.567697 65.8621325,198.148576 C71.0772151,190.971126 75.7283628,183.341335 79.7352139,175.300261 C72.104019,172.400575 64.7949724,168.822202 57.8887866,164.667963 C59.7209612,163.310589 61.5131304,161.891452 63.2445898,160.431257 C105.36741,180.133187 151.134928,180.133187 192.754523,160.431257 C194.506336,161.891452 196.298154,163.310589 198.110326,164.667963 C191.183787,168.842556 183.854737,172.420929 176.223542,175.320965 C180.230393,183.341335 184.861538,190.991831 190.096624,198.16893 C211.238746,191.588051 232.743023,181.531619 254.911949,164.955721 C260.227747,108.668201 245.831087,59.8662432 216.856339,16.5966031 Z M85.4738752,135.09489 C72.8290281,135.09489 62.4592217,123.290155 62.4592217,108.914901 C62.4592217,94.5396472 72.607595,82.7145587 85.4738752,82.7145587 C98.3405064,82.7145587 108.709962,94.5189427 108.488529,108.914901 C108.508531,123.290155 98.3405064,135.09489 85.4738752,135.09489 Z M170.525237,135.09489 C157.88039,135.09489 147.510584,123.290155 147.510584,108.914901 C147.510584,94.5396472 157.658606,82.7145587 170.525237,82.7145587 C183.391518,82.7145587 193.761324,94.5189427 193.539891,108.914901 C193.539891,123.290155 183.391518,135.09489 170.525237,135.09489 Z" fill="currentColor" fill-rule="nonzero"></path></svg></a></div></div></footer><script>window.initial_dioxus_hydration_data="hIEY9oEY9YEY9YEY9Q==";</script>
  </div>
  <script>
    window.addEventListener('scroll', () => {
      let scrollTop = window.scrollY;
      let winHeight = window.document.documentElement.scrollHeight - window.document.documentElement.clientHeight;
      let new_scroll = Math.min(Math.max(scrollTop, 0) / Math.max(winHeight, 1), 1);

      document.body.style.setProperty('--scroll', new_scroll);
    }, false);
  </script>
<script>
            // We can't use a module script here because we need to start the script immediately when streaming
            import("/./assets/floneum-site-ce954da0ae683c0e.js").then(
                ({ default: init }) => {
                init("/./assets/floneum-site_bg-d8c3438104baad75.wasm").then((wasm) => {
                    if (wasm.__wbindgen_start == undefined) {
                    wasm.main();
                    }
                });
                }
            );
            </script>
            
            </body>

</html>